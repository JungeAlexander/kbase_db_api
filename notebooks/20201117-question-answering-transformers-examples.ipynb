{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering using ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://huggingface.co/transformers/task_summary.html#extractive-question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", return_dict=True)\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "def get_answer(question, text, tokenizer, model):\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    model_output = model(**inputs)\n",
    "    answer_start_scores = model_output.start_logits\n",
    "    answer_end_scores = model_output.end_logits\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    answer = get_answer(question, text, tokenizer, model)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"\"\"\n",
    "But that doesn't make it a good fit for every data processing need. \n",
    "And when you outgrow Excel, a really good option for a next step is Python and \n",
    "the data science tech stack: Pandas, Jupyter, and friends.\n",
    "\"\"\"\n",
    "questions = [\"What is the data science tech stack?\",\n",
    "             \"What is good option for a next step when you outgrow Excel?\"]\n",
    "\n",
    "for question in questions:\n",
    "    answer = get_answer(question, text, tokenizer, model)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"\"\"\n",
    "Note this is only works around the way this bug crashes NumPy \n",
    "(technically, in OpenBLAS which is shipped with NumPy), \n",
    "and may not fix all your problems related to this bug, \n",
    "Microsoftâ€™s help is needed to do that.\n",
    "\"\"\"\n",
    "questions = [\"What is shipped with NumPy?\",\n",
    "             \"Whose help is needed to fix all your problems?\"]\n",
    "\n",
    "for question in questions:\n",
    "    answer = get_answer(question, text, tokenizer, model)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
